#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import hashlib
import sys
import pandas as pd
import numpy as np
import joblib
from fuzzywuzzy import process, fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import warnings
import os
import jieba

# ç¯å¢ƒé…ç½®
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ç¦ç”¨TensorFlowæ—¥å¿—
warnings.filterwarnings("ignore")
pd.set_option('display.unicode.ambiguous_as_wide', True)
pd.set_option('display.unicode.east_asian_width', True)


# ======================== é…ç½®åŒºåŸŸ ========================
class Config:
    # ç¼“å­˜é…ç½®
    CACHE_DIR = Path("cache")
    TFIDF_CACHE = CACHE_DIR / "tfidf_model.pkl"
    DATA_CACHE = CACHE_DIR / "medical_data.feather"

    DATA_22_PATH = './data/22.csv'

    # æ€§èƒ½é…ç½®
    MAX_RESULTS = 10  # æœ€å¤§è¿”å›ç»“æœæ•°
    MIN_QUERY_LEN = 2  # æœ€å°æŸ¥è¯¢é•¿åº¦


# ======================== æ ¸å¿ƒåŠŸèƒ½ ========================
def _tokenize_zh(text):
    """ä¸­æ–‡åˆ†è¯ä¼˜åŒ–"""
    if text is None:
        return []
    return [word for word in jieba.cut(text) if word.strip()]


class cfi22:
    def __init__(self):
        self.config = Config()
        self._init_cache()
        self.data = self._load_data()
        self._init_models()
        self.ex_kv = self.build_ex_kv()

    def _init_cache(self):
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•"""
        self.config.CACHE_DIR.mkdir(exist_ok=True)

    def _load_data(self):
        """æ™ºèƒ½åŠ è½½æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        try:
            if self.config.DATA_CACHE.exists():
                print("ğŸ”„ ä»ç¼“å­˜åŠ è½½æ•°æ®...", end="")
                df = pd.read_feather(self.config.DATA_CACHE)
                print("\râœ… ç¼“å­˜æ•°æ®åŠ è½½å®Œæˆï¼ˆ{}æ¡è®°å½•ï¼‰".format(len(df)))
            else:
                print("ğŸ”„ ä»æœ¬åœ°CSVæ–‡ä»¶åŠ è½½æ•°æ®...", end="")
                df = pd.read_csv(self.config.DATA_22_PATH)
                df.to_feather(self.config.DATA_CACHE)  # ä¿å­˜ä¸ºé«˜æ€§èƒ½featheræ ¼å¼
                print("\râœ… æœ¬åœ°CSVæ–‡ä»¶åŠ è½½å®Œæˆï¼ˆ{}æ¡è®°å½•ï¼‰".format(len(df)))

            # **é˜²æ­¢ None å€¼**
            df['sku_ex'] = df['sku_ex'].fillna('').astype(str)
            df['use_to'] = df['use_to'].fillna('').astype(str)
            df['index-complex'] = df['sku_ex'] + df['use_to']
            return df

        except Exception as e:
            print(f"\nâŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            print("âš ï¸ ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®è¿è¡Œ")
            return self._get_sample_data()

    def _get_sample_data(self):
        """åº”æ€¥ç¤ºä¾‹æ•°æ®"""
        return pd.DataFrame({
            'name': ['è¡€ç³–æ£€æµ‹ä»ª', 'å¼•æµå¯¼ç®¡', 'è¶…å£°è¯Šæ–­ä»ª', 'å¿ƒè„æ”¯æ¶', 'åŒ»ç”¨å£ç½©'],
            'code': ['IVD-001', 'DEV-002', 'DEV-003', 'IMP-004', 'PRO-005'],
            'level': [3, 2, 3, 3, 1],
            'sku_ex': [
                'ç”¨äºè¡€ç³–æ°´å¹³æ£€æµ‹',
                'ç”¨äºæœ¯åå¼•æµ',
                'ç”¨äºåŒ»å­¦å½±åƒè¯Šæ–­',
                'ç”¨äºå¿ƒè¡€ç®¡æ‰‹æœ¯',
                'ç”¨äºä¸ªäººé˜²æŠ¤'
            ]
        })

    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            if self.config.TFIDF_CACHE.exists():
                print("ğŸ”„ åŠ è½½ç¼“å­˜æ¨¡å‹...", end="")
                if self.config.TFIDF_CACHE.is_file():
                    self.tfidf = joblib.load(self.config.TFIDF_CACHE)
                    print("\râœ… ç¼“å­˜æ¨¡å‹åŠ è½½å®Œæˆ")
                else:
                    raise FileNotFoundError(f"{self.config.TFIDF_CACHE} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶")
            else:
                print("ğŸ”„ è®­ç»ƒTF-IDFæ¨¡å‹...", end="")
                # åˆå§‹åŒ–å¹¶æ‹Ÿåˆ TF-IDF æ¨¡å‹
                self.tfidf = TfidfVectorizer(
                    tokenizer=_tokenize_zh,
                    ngram_range=(1, 2)
                )
                self.tfidf.fit(self.data['index-complex'])
                joblib.dump(self.tfidf, self.config.TFIDF_CACHE)
                print("\râœ… æ¨¡å‹è®­ç»ƒå¹¶ç¼“å­˜å®Œæˆ")

            # é¢„è®¡ç®—çŸ©é˜µ
            self.tfidf_matrix = self.tfidf.transform(self.data['index-complex'])
            print("\râœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            print(f"\nâŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            sys.exit(1)

    def build_ex_kv(self):
        ex_kv = {}
        for index, row in self.data.iterrows():
            ex_kv[hashlib.md5(
                (row['sku_ex'] + row['use_to']).encode(
                    encoding='UTF-8')).hexdigest()] = row
        return ex_kv

    def fuzzy_search(self, query, top_n=5):
        """ä¼˜åŒ–æ¨¡ç³Šæœç´¢"""
        sku_ex = self.data['index-complex'].tolist()
        matches = process.extract(query, sku_ex, scorer=fuzz.token_set_ratio, limit=top_n)

        results = []
        for name, score in matches:
            md5 = hashlib.md5(name.encode(encoding='UTF-8')).hexdigest()
            row = self.ex_kv[md5]

            results.append({
                'name': row['name'],
                'code': row['code'],
                'level': row['level'],
                'score': score / 100,  # å½’ä¸€åŒ–åˆ°0-1
                'exp': row.get('sku_ex', ''),
                'description': row.get('use_to', '')
            })
        return results

    def semantic_search(self, query, top_n=5):
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢"""
        query_vec = self.tfidf.transform([query])
        cos_sim = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
        top_indices = np.argsort(cos_sim)[-top_n:][::-1]

        results = []
        for idx in top_indices:
            row = self.data.iloc[idx]
            results.append({
                'name': row['name'],
                'code': row['code'],
                'level': row['level'],
                'score': float(cos_sim[idx]),
                'exp': row.get('sku_ex', ''),
                'description': row.get('use_to', '')
            })
        return results
